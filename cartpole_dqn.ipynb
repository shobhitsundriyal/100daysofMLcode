{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole-dqn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG3DGYqGNFZWQSJBDOqped"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "THWyvuswASJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eLDq31lBLC9",
        "colab_type": "text"
      },
      "source": [
        "##### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y0DR2WNAWhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwan9bAJBhwX",
        "colab_type": "code",
        "outputId": "b57eca73-56e6-4f48-fd37-6d1717aa6fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "state_size"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6gYhOunBqrL",
        "colab_type": "code",
        "outputId": "c81537c2-3224-47f3-943a-4f2e56d9c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "action_size = env.action_space.n\n",
        "action_size"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XErNVLGgB20e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0UvZldkCB4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_episodes = 1000\n",
        "output_dir = 'output_dir/model/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJapCOa0CSxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArSmiMXNCj5D",
        "colab_type": "text"
      },
      "source": [
        "##### Define agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S6kYJWtCfW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  \n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    \n",
        "    self.memory = deque(maxlen=2000)      # interested in lastest 2000 entries\n",
        "    self.gamma = 0.95                     # discount factor\n",
        "    self.epsilon = 1.0                    #exploration/exploit rate initially only explore\n",
        "    self.epsilon_decay = 0.995            #shift exploration to eploitation gradually\n",
        "    self.epsilon_min = 0.01               # still explore 1% of time evven after learning exploit\n",
        "    \n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "    \n",
        "  def _build_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    \n",
        "    model.add(Dense(self.action_size, activation='linear'))  #linear activation coz we wantmodel to output direct actions\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "  \n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "  def act(self, state):\n",
        "    if np.random.randn() <= self.epsilon:              # Exploration\n",
        "      return random.randrange(self.action_size)\n",
        "    # Exploit\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "  \n",
        "  def replay(self, batch_size): #revise this method\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "    \n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "      target = reward\n",
        "      \n",
        "      if not done:\n",
        "        target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "        \n",
        "      target_f = self.model.predict(state)\n",
        "      target_f[0][action] = target\n",
        "      \n",
        "      self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "      \n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon = self.epsilon * self.epsilon_decay\n",
        "        \n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "    \n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LKdqIrKO3Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = DQNAgent(state_size, action_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv1_zXutPaXj",
        "colab_type": "text"
      },
      "source": [
        "##### Interact with environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25hAMrLXShB_",
        "colab_type": "text"
      },
      "source": [
        "socastic gradient assent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95bGktVjPU_b",
        "colab_type": "code",
        "outputId": "3e917231-bde7-40c1-b3db-35092eeb6ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "done = False\n",
        "\n",
        "for e in range(n_episodes):\n",
        "  \n",
        "  state = env.reset()\n",
        "  #print(state)\n",
        "  state = np.reshape(state, [1, state_size])\n",
        "  \n",
        "  for time in range(500): # cartpole v0 only has 200 time steps any thing greater than 200 is fine\n",
        "    \n",
        "    #env.render()\n",
        "    action = agent.act(state)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    reward = reward if not done else -10\n",
        "    next_state = np.reshape(next_state, [1, state_size])\n",
        "    \n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    \n",
        "    state = next_state\n",
        "    \n",
        "    if done:\n",
        "      print('Episode: {}/{}, Score: {}, e: {:.2}'.format(e, n_episodes, time, agent.epsilon))\n",
        "      print(agent.epsilon_decay *  agent.epsilon)\n",
        "      break\n",
        "      \n",
        "  if len(agent.memory) > batch_size:\n",
        "    agent.replay(batch_size)\n",
        "    \n",
        "  if e%50 == 0:\n",
        "    agent.save(output_dir + 'weigths_' + '{:04d}'.format(e) + '.hdf5')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0/1000, Score: 21, e: 1.0\n",
            "0.995\n",
            "Episode: 1/1000, Score: 16, e: 1.0\n",
            "0.995\n",
            "Episode: 2/1000, Score: 12, e: 0.85\n",
            "0.8475428503023453\n",
            "Episode: 3/1000, Score: 10, e: 0.73\n",
            "0.7219385759785162\n",
            "Episode: 4/1000, Score: 20, e: 0.62\n",
            "0.6149486215357263\n",
            "Episode: 5/1000, Score: 19, e: 0.53\n",
            "0.5238143793828016\n",
            "Episode: 6/1000, Score: 11, e: 0.45\n",
            "0.446186062443672\n",
            "Episode: 7/1000, Score: 15, e: 0.38\n",
            "0.3800621177172763\n",
            "Episode: 8/1000, Score: 16, e: 0.33\n",
            "0.3237376186352221\n",
            "Episode: 9/1000, Score: 18, e: 0.28\n",
            "0.2757603055760701\n",
            "Episode: 10/1000, Score: 14, e: 0.24\n",
            "0.23489314109365644\n",
            "Episode: 11/1000, Score: 16, e: 0.2\n",
            "0.2000824143909432\n",
            "Episode: 12/1000, Score: 8, e: 0.17\n",
            "0.17043057265153258\n",
            "Episode: 13/1000, Score: 15, e: 0.15\n",
            "0.1451730787173275\n",
            "Episode: 14/1000, Score: 17, e: 0.12\n",
            "0.12365869841532712\n",
            "Episode: 15/1000, Score: 18, e: 0.11\n",
            "0.10533270926593409\n",
            "Episode: 16/1000, Score: 9, e: 0.09\n",
            "0.08972259762946533\n",
            "Episode: 17/1000, Score: 16, e: 0.077\n",
            "0.07642587550895225\n",
            "Episode: 18/1000, Score: 20, e: 0.065\n",
            "0.06509970288011008\n",
            "Episode: 19/1000, Score: 39, e: 0.056\n",
            "0.0554520479727078\n",
            "Episode: 20/1000, Score: 12, e: 0.047\n",
            "0.047234157581800176\n",
            "Episode: 21/1000, Score: 13, e: 0.04\n",
            "0.04023414326483323\n",
            "Episode: 22/1000, Score: 12, e: 0.034\n",
            "0.034271518052411715\n",
            "Episode: 23/1000, Score: 10, e: 0.029\n",
            "0.029192542808371146\n",
            "Episode: 24/1000, Score: 11, e: 0.025\n",
            "0.024866262250633264\n",
            "Episode: 25/1000, Score: 19, e: 0.021\n",
            "0.021181128426399323\n",
            "Episode: 26/1000, Score: 13, e: 0.018\n",
            "0.018042124582040707\n",
            "Episode: 27/1000, Score: 26, e: 0.015\n",
            "0.015368315270123408\n",
            "Episode: 28/1000, Score: 12, e: 0.013\n",
            "0.013090759526015528\n",
            "Episode: 29/1000, Score: 27, e: 0.011\n",
            "0.011150733307840981\n",
            "Episode: 30/1000, Score: 14, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 31/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 32/1000, Score: 21, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 33/1000, Score: 59, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 34/1000, Score: 12, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 35/1000, Score: 26, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 36/1000, Score: 16, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 37/1000, Score: 14, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 38/1000, Score: 14, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 39/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 40/1000, Score: 14, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 41/1000, Score: 12, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 42/1000, Score: 12, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 43/1000, Score: 23, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 44/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 45/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 46/1000, Score: 15, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 47/1000, Score: 18, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 48/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 49/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 50/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 51/1000, Score: 12, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 52/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 53/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 54/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 55/1000, Score: 9, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 56/1000, Score: 13, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 57/1000, Score: 9, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 58/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 59/1000, Score: 15, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 60/1000, Score: 14, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 61/1000, Score: 10, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 62/1000, Score: 12, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 63/1000, Score: 15, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 64/1000, Score: 8, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 65/1000, Score: 9, e: 0.01\n",
            "0.009936519429207103\n",
            "Episode: 66/1000, Score: 7, e: 0.01\n",
            "0.009936519429207103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-dde5da02e8b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-1a3ad59bda7a>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KYfDrotZ3_f",
        "colab_type": "text"
      },
      "source": [
        "To render openai gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9opu6zpTpWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DotCtuXPZ_5_",
        "colab_type": "code",
        "outputId": "98fa8dc4-a977-46b0-9a20-8d604b8f4e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyq-xiVel1in",
        "colab_type": "text"
      },
      "source": [
        "argmax => index of maxval</br>\n",
        "amax => max value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gUE5qoFaGXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}